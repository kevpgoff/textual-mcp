# Textual MCP Server Configuration
validators:
  strict_mode: false
  cache_enabled: true
  max_file_size: 1048576  # 1MB
  timeout: 30

# New search configuration using VectorDB
search:
  # Enable automatic indexing on first use
  auto_index: true

  # Embedding model configuration
  # Options: 'fast', 'TaylorAI/bge-micro-v2', 'BAAI/bge-small-en-v1.5', etc.
  embeddings_model: 'BAAI/bge-base-en-v1.5'

  # Optional: Path to persist the vector database
  persist_path: './data/textual_docs.db'

  # Document processing settings
  chunk_size: 200
  chunk_overlap: 20

  # Optional GitHub token for higher API rate limits
  # github_token: ${GITHUB_TOKEN}  # Uncomment and set, or use GITHUB_TOKEN env var

  # Search result settings
  default_limit: 10
  similarity_threshold: 0.7

  # Chunking strategy
  chunking_strategy: 'chonkie'  # 'manual' | 'chonkie'

  # Chonkie-specific settings
  chonkie:
    default_chunker: 'semantic'  # 'token' | 'sentence' | 'semantic' | 'recursive'

    # Model for semantic chunking
    embedding_model: 'all-mpnet-base-v2'

    # Content-type specific settings
    content_types:
      code:
        chunker: 'experimental_code'
        chunk_size: 512
        language: 'python'

      api:
        chunker: 'semantic'
        chunk_size: 768
        min_chunk_size: 256

      guide:
        chunker: 'recursive'
        recipe: 'markdown'
        chunk_size: 512

      css_reference:
        chunker: 'semantic'
        chunk_size: 384
        min_sentences: 2

performance:
  cache_size: 100
  timeout: 30
  max_concurrent_requests: 10

logging:
  level: INFO
  format: json
  file: textual-mcp.log

features:
  experimental: false
  plugins_enabled: true
