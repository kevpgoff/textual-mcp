# Textual MCP Server Configuration
validators:
  strict_mode: false
  cache_enabled: true
  max_file_size: 1048576  # 1MB
  timeout: 30

# New search configuration using VectorDB
search:
  # Enable automatic indexing on first use
  auto_index: true

  # Embedding model configuration
  # Options: 'fast', 'TaylorAI/bge-micro-v2', 'BAAI/bge-small-en-v1.5', etc.
  embeddings_model: 'TaylorAI/bge-micro-v2'

  # Optional: Path to persist the vector database
  persist_path: './data/textual_docs.db'

  # Document processing settings
  chunk_size: 200
  chunk_overlap: 20

  # Optional GitHub token for higher API rate limits
  # github_token: ${GITHUB_TOKEN}  # Uncomment and set, or use GITHUB_TOKEN env var

  # Search result settings
  default_limit: 10
  similarity_threshold: 0.7

performance:
  cache_size: 100
  timeout: 30
  max_concurrent_requests: 10

logging:
  level: INFO
  format: json
  file: textual-mcp.log

features:
  experimental: false
  plugins_enabled: true
